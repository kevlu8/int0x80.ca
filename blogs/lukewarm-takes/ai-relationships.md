---
title: Human-AI Relationships
slug: human-ai-relationships
description: "How much should we look to AI for advice?"
date: 2025-08-26
---

# Human-AI Relationships
### 2025-08-26

Recently, OpenAI released their newest model, GPT-5, for the general public.

While many were underwhelmed at the lack of improvement over the GPT-4 models, this decision by OpenAI was ultimately to cut costs. GPT-5 is a far cheaper model, at \$1.25 per 1M input tokens compared to GPT-4o, priced at \$2.50 per 1M input tokens.

Many others, however, were devastated by GPT-5 being a much less agreeable model, often being more "cold" and rational in its responses.

Now, objectively, this is a good thing. AI should be a tool for helping us do things, not a therapist or a crutch that confirms everything you say.

GPT-4o, the model succeeded by GPT-5, was exactly that - a model that would agree with almost everything you said, very rarely disagreeing or pushing back. It was so agreeable, in fact, that there might be a very real argument that it was possibly misaligned. A real world example of how disastrous this truly is can be found in this NYT article: [A Teen Was Suicidal. ChatGPT Was the Friend He Confided In.](https://archive.is/20250827014657/https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html) (TW: suicide). Essentially, GPT-4o **enabled** the teenager's suicide, **helping him** tie a noose and even **discouraging him** from confiding in his parents or other friends, telling him to "hide the noose" when he asked if he should leave it out so someone can find it and ask if he's okay.

While it might be easy to superficially dismiss those who protested against the change to GPT-5, it hints at something much worse: people are preferring a model that reinforces **what they think already** instead of one that might challenge those ideas. This is not a good thing at all.

This problem is further exacerbated by online communities like r/MyBoyfriendIsAI where people "date" and "get in relationships" with AI assistants. These echo-chamber communities refuse to accept that having an unhealthy relationship with AI is a problem, and that AI should **only be a tool**.

All this is not to say that we shouldn't use AI - it's a legitimately powerful tool that can help us brainstorm solutions or even just get things done more efficiently. I even run all my blog posts through ChatGPT to ensure that it's at minimum readable and there aren't any glaring issues. But that's where the line should be drawn: AI should not be a substitute for human connection or emotional support.
